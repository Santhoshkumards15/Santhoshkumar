{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhyPI3FKhP95"
   },
   "source": [
    "# Heart Disease Prediction\n",
    "\n",
    "In this machine learning project, I have collected the dataset from Kaggle (https://www.kaggle.com/ronitf/heart-disease-uci) and I will be using Machine Learning to make predictions on whether a person is suffering from Heart Disease or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRjKpdA-hP9-"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Let's first import all the necessary libraries. I'll use `numpy` and `pandas` to start with. For visualization, I will use `pyplot` subpackage of `matplotlib`, use `rcParams` to add styling to the plots and `rainbow` for colors. For implementing Machine Learning models and processing of data, I will use the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRrEJrjXhP9-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpxeNTqEhP-A"
   },
   "source": [
    "For processing the data, I'll import a few libraries. To split the available dataset for testing and training, I'll use the `train_test_split` method. To scale the features, I am using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyqVAg-ihP-A"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qAKvleRhP-A"
   },
   "source": [
    "Next, I'll import all the Machine Learning algorithms I will be using.\n",
    "1. K Neighbors Classifier\n",
    "2. Support Vector Classifier\n",
    "3. Decision Tree Classifier\n",
    "4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9aDX8BOhP-B"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WOxcygDhP-B"
   },
   "source": [
    "### Import dataset\n",
    "\n",
    "Now that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file `dataset.csv`. I'll use the pandas `read_csv` method to read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjMLEOdDhP-C"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/content/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZANjA2V8hP-D"
   },
   "source": [
    "The dataset is now loaded into the variable `dataset`. I'll just take a glimpse of the data using the `desribe()` and `info()` methods before I actually start processing and visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21rr5BUohP-D",
    "outputId": "042451f4-006c-4ad2-90c5-9c2019a29f49"
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oFoXSYlhP-E"
   },
   "source": [
    "Looks like the dataset has a total of 303 rows and there are no missing values. There are a total of `13 features` along with one target value which we wish to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "6yTU9pxFhP-E",
    "outputId": "755ab184-85e5-46a2-cce3-a7527bd23d25"
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTnJnyD3hP-F"
   },
   "source": [
    "The scale of each feature column is different and quite varied as well. While the maximum for `age` reaches 77, the maximum of `chol` (serum cholestoral) is 564."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESkTFlMzhP-F"
   },
   "source": [
    "### Understanding the data\n",
    "\n",
    "Now, we can use visualizations to better understand our data and then look at any processing we might want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "wUYAhXPnhP-F",
    "outputId": "63b9b475-b295-47ec-c262-0608377d437b"
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20, 14\n",
    "plt.matshow(dataset.corr())\n",
    "plt.yticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.xticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kG7aTjbhP-G"
   },
   "source": [
    "Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive.\n",
    "Next, I'll take a look at the histograms for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DVY2Fs7ThP-G",
    "outputId": "ccb18290-2b28-464e-a3e0-c90365d19822"
   },
   "outputs": [],
   "source": [
    "dataset.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRakkyALhP-G"
   },
   "source": [
    "Taking a look at the histograms above, I can see that each feature has a different range of distribution. Thus, using scaling before our predictions should be of great use. Also, the categorical features do stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPpZNyebhP-G"
   },
   "source": [
    "It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "ABnmbiCfhP-H",
    "outputId": "3e7565ac-e408-4225-f49b-49f9ec095c83"
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 8,6\n",
    "plt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Target Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of each Target Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaqnqDUMhP-H"
   },
   "source": [
    "The two classes are not exactly 50% each but the ratio is good enough to continue without dropping/increasing our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UTEldsMhP-H"
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "After exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models.\n",
    "First, I'll use the `get_dummies` method to create dummy columns for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoaEGh1EhP-I"
   },
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0FXvuaThP-I"
   },
   "source": [
    "Now, I will use the `StandardScaler` from `sklearn` to scale my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIDLpLOthP-I"
   },
   "outputs": [],
   "source": [
    "standardScaler = StandardScaler()\n",
    "columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTM98EZ1hP-I"
   },
   "source": [
    "The data is not ready for our Machine Learning application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3z61Z-ZhP-I"
   },
   "source": [
    "### Machine Learning\n",
    "\n",
    "I'll now import `train_test_split` to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R2vE52phP-I"
   },
   "outputs": [],
   "source": [
    "y = dataset['target']\n",
    "X = dataset.drop(['target'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJLQOR7MhP-J"
   },
   "source": [
    "#### K Neighbors Classifier\n",
    "\n",
    "The classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujj9IoQ_hP-J"
   },
   "outputs": [],
   "source": [
    "knn_scores = []\n",
    "for k in range(1,21):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    knn_scores.append(knn_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhzx8kkwhP-J"
   },
   "source": [
    "I have the scores for different neighbor values in the array `knn_scores`. I'll now plot it and see for which value of K did I get the best scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "WoaC_zL4hP-J",
    "outputId": "29d46cfb-8a67-483f-b2b9-cd1593be2a13"
   },
   "outputs": [],
   "source": [
    "plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\n",
    "for i in range(1,21):\n",
    "    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\n",
    "plt.xticks([i for i in range(1, 21)])\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('K Neighbors Classifier scores for different K values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbrrK3BxhP-J"
   },
   "source": [
    "From the plot above, it is clear that the maximum score achieved was `0.87` for the 8 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69QB5hlehP-J",
    "outputId": "1395f363-bb86-4e3b-b006-b5c3c0ed4503"
   },
   "outputs": [],
   "source": [
    "print(\"The score for K Neighbors Classifier is {}% with {} nieghbors.\".format(knn_scores[7]*100, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xad5YS4rhP-K"
   },
   "source": [
    "#### Support Vector Classifier\n",
    "\n",
    "There are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfCkmCKphP-K"
   },
   "outputs": [],
   "source": [
    "svc_scores = []\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "for i in range(len(kernels)):\n",
    "    svc_classifier = SVC(kernel = kernels[i])\n",
    "    svc_classifier.fit(X_train, y_train)\n",
    "    svc_scores.append(svc_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcaXLqUghP-K"
   },
   "source": [
    "I'll now plot a bar plot of scores for each kernel and see which performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "XOcqmGjyhP-K",
    "outputId": "3f98ad36-99e0-4fbc-8f17-f5c50e48ad95"
   },
   "outputs": [],
   "source": [
    "colors = rainbow(np.linspace(0, 1, len(kernels)))\n",
    "plt.bar(kernels, svc_scores, color = colors)\n",
    "for i in range(len(kernels)):\n",
    "    plt.text(i, svc_scores[i], svc_scores[i])\n",
    "plt.xlabel('Kernels')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Support Vector Classifier scores for different kernels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naIqHi6MhP-L"
   },
   "source": [
    "The `linear` kernel performed the best, being slightly better than `rbf` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LnEXwSXhP-L",
    "outputId": "a3faf618-c45a-4c13-9755-a89e6d701c57"
   },
   "outputs": [],
   "source": [
    "print(\"The score for Support Vector Classifier is {}% with {} kernel.\".format(svc_scores[0]*100, 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yezD7mnhP-L"
   },
   "source": [
    "#### Decision Tree Classifier\n",
    "\n",
    "Here, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of `max_features` and see which returns the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLsMYkd7hP-L"
   },
   "outputs": [],
   "source": [
    "dt_scores = []\n",
    "for i in range(1, len(X.columns) + 1):\n",
    "    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "    dt_scores.append(dt_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiRecTthhP-L"
   },
   "source": [
    "I selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "5pQUNtWfhP-L",
    "outputId": "0b4c73c8-61f3-422b-f828-9f2b4b1f0994"
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\n",
    "for i in range(1, len(X.columns) + 1):\n",
    "    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\n",
    "plt.xticks([i for i in range(1, len(X.columns) + 1)])\n",
    "plt.xlabel('Max features')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Decision Tree Classifier scores for different number of maximum features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG-WIwCxhP-M"
   },
   "source": [
    "The model achieved the best accuracy at three values of maximum features, `2`, `4` and `18`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43lcETdbhP-M",
    "outputId": "58cdc518-8453-4d60-9cd2-29fde3e63554"
   },
   "outputs": [],
   "source": [
    "print(\"The score for Decision Tree Classifier is {}% with {} maximum features.\".format(dt_scores[17]*100, [2,4,18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgdI_jlBhP-M"
   },
   "source": [
    "#### Random Forest Classifier\n",
    "\n",
    "Now, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDQgPRhlhP-M"
   },
   "outputs": [],
   "source": [
    "rf_scores = []\n",
    "estimators = [10, 100, 200, 500, 1000]\n",
    "for i in estimators:\n",
    "    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    rf_scores.append(rf_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC8oLVp1hP-M"
   },
   "source": [
    "The model is trained and the scores are recorded. Let's plot a bar plot to compare the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "_Whn2FX4hP-N",
    "outputId": "c6e738da-c20a-48be-ba00-daeb02fb47db",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "colors = rainbow(np.linspace(0, 1, len(estimators)))\n",
    "plt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\n",
    "for i in range(len(estimators)):\n",
    "    plt.text(i, rf_scores[i], rf_scores[i])\n",
    "plt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Random Forest Classifier scores for different number of estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXU60S89hP-N"
   },
   "source": [
    "The maximum score is achieved when the total estimators are 100 or 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2muJ9OCthP-N",
    "outputId": "e4dc5d97-51da-4ab5-c377-72f7f1c19e84"
   },
   "outputs": [],
   "source": [
    "print(\"The score for Random Forest Classifier is {}% with {} estimators.\".format(rf_scores[1]*100, [100, 500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD_D5YgUhP-N"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, I used Machine Learning to predict whether a person is suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. \n",
    "I then applied four Machine Learning algorithms, `K Neighbors Classifier`, `Support Vector Classifier`, `Decision Tree Classifier` and `Random Forest Classifier`. I varied parameters across each model to improve their scores.\n",
    "In the end, `K Neighbors Classifier` achieved the highest score of `87%` with `8 nearest neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOMS5coGiexP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Heart Disease Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
